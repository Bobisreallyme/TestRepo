{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPxdY59kEJ6fGDD+klL36Cl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobisreallyme/TestRepo/blob/main/Cleaned_Embedding_Generation_06122024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYERSsvYRybo"
      },
      "outputs": [],
      "source": [
        "!pip install odfpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "z08UU2PkSJeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = r'/content/drive/MyDrive/ModelTrained/ModelandTokenizer10/Model_from_concatentation_4_26_2024_10'\n",
        "streams_of_interest = r'YourStreamsOfInterest.ods'\n",
        "database_of_interest = r'YourDatabase.db'"
      ],
      "metadata": {
        "id": "Dx1dGCJfu_gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#IMPORT THE PRETRAINED BERT MODEL FOR GENERATING EMBEDDINGS:\n",
        "model = AutoModel.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model.eval()\n",
        "\n",
        "#GET THE STREAMS OF INTEREST:\n",
        "streamdf = pd.read_excel(streams_of_interest)\n",
        "url_list = streamdf.iloc[:,2].tolist()\n",
        "name_list = streamdf.iloc[:,1].tolist()\n",
        "streams_of_interest = []\n",
        "for i in range(93,100+93,2):\n",
        "  streams_of_interest.append(url_list[i])\n",
        "\n",
        "#DOUBLE CHECK NAMES OF ALL COLUMNS\n",
        "con = sqlite3.connect(database_of_interest)\n",
        "dbcursor = con.cursor()\n",
        "query = \"PRAGMA TABLE_INFO(DBdb)\"\n",
        "dbcursor.execute(query)\n",
        "print(dbcursor.fetchall())\n",
        "\n",
        "#ADDITIONAL CHECKS OF TYPES OF AUDIENCES\n",
        "query = f\"SELECT DISTINCT ListofBadges FROM DBdb WHERE URL = ?\"\n",
        "df = pd.read_sql(query,con,params = [streams_of_interest[40]])\n",
        "df\n"
      ],
      "metadata": {
        "id": "chFmHMVPSMHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for:\n",
        "1. Identifying all donors and non-donors in a stream of interest and aggregating all messages an individual sent together\n",
        "2. Applying (1) to all streams of interest and producing a dataframe containing this aggregated data, along with the timescale of the stream\n",
        "3. Using the output of (2) and running this data through the model to generate embeddings, then converting these embeddings to a form that the transformer can use"
      ],
      "metadata": {
        "id": "PdyFr_1KagQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINE FUNCTION TO GO THROUGH STREAM OF INTEREST AND CONVERT IT TO A SET OF EMBEDDINGS\n",
        "def message_extractor(stream):\n",
        "  #GET THE LAST MESSAGE TIME\n",
        "  query = f\"\"\"\n",
        "  SELECT ListofTimes\n",
        "  FROM DBdb\n",
        "  WHERE URL = (?)\n",
        "  \"\"\"\n",
        "  df = pd.read_sql(query, con, params=[stream])\n",
        "  if not df.empty:\n",
        "      df[\"ListofTimes\"] = df[\"ListofTimes\"].astype(float)\n",
        "      last_message_time = df[\"ListofTimes\"].max()\n",
        "  else:\n",
        "      last_message_time = None\n",
        "\n",
        "  #EXTRACT AND FORMAT ALL THE MESSAGES\n",
        "\n",
        "  query = f\"\"\"\n",
        "  SELECT ListofAuthors, json_group_array(json_object('message', ListofMessages, 'time', ListofTimes)) as messages_and_times\n",
        "  FROM DBdb\n",
        "  WHERE URL = (?) AND ListofMessages != ''\n",
        "  GROUP BY ListofAuthors\n",
        "  \"\"\"\n",
        "\n",
        "  df_all = pd.read_sql(query,con,params = [stream])\n",
        "\n",
        "  #FIND DONORS\n",
        "  paid_message_types = ['paid_message','paid_sticker','sponsorships_gift_purchase_announcement']\n",
        "  placeholders = ', '.join('?' for _ in paid_message_types)\n",
        "\n",
        "  query_donors = f\"\"\"\n",
        "  SELECT DISTINCT ListofAuthors\n",
        "  FROM DBdb\n",
        "  WHERE URL = (?)\n",
        "  AND TypeofMessages IN ({placeholders})\n",
        "  \"\"\"\n",
        "\n",
        "  df_donors_check = pd.read_sql(query_donors, con, params=[stream] + paid_message_types)\n",
        "  donors = set(df_donors_check['ListofAuthors'])\n",
        "\n",
        "  df_all['is_donor'] = df_all['ListofAuthors'].apply(lambda x: x in donors)\n",
        "  df_donors = df_all[df_all['is_donor'] == True]\n",
        "  df_non_donors = df_all[df_all['is_donor'] == False]\n",
        "\n",
        "  return df_donors, df_non_donors, last_message_time\n",
        "\n",
        "#DEFINE FUNCTION THAT APPLIES THIS TO ALL RELEVENT STREAMS\n",
        "\n",
        "def data_aggregator(streams_of_interest):\n",
        "  all_donors = []\n",
        "  all_non_donors = []\n",
        "  all_last_message_time = []\n",
        "  for stream in streams_of_interest:\n",
        "    df_donors, df_non_donors, last_message_time = message_extractor(stream)\n",
        "    df_donors['final_time'] = last_message_time\n",
        "    df_non_donors['final_time'] = last_message_time\n",
        "    all_donors.append(df_donors)\n",
        "    all_non_donors.append(df_non_donors)\n",
        "    all_last_message_time.append(last_message_time)\n",
        "    all_donors_df = pd.concat(all_donors, ignore_index=True)\n",
        "    all_non_donors_df = pd.concat(all_non_donors, ignore_index=True)\n",
        "\n",
        "  return all_donors_df, all_non_donors_df, all_last_message_time\n",
        "\n",
        "#DEFINE FUNCTION THAT APPLIES THIS TO ALL RELEVENT STREAMS\n",
        "def embedding_generator(df,max_length):\n",
        "  dataset = df.copy()\n",
        "  dataset['Embeddings'] = \"\"\n",
        "  dataset['Mask'] = \"\"\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    padding_gen = np.zeros((max_length,768+1+1))\n",
        "    #The mask must also include the CLS token that will be added on later\n",
        "    mask_gen = np.zeros((max_length+1))\n",
        "    print(i)\n",
        "    last_message_time = dataset.iloc[i]['final_time']\n",
        "    message_time_output = json.loads(dataset.iloc[i]['messages_and_times'])\n",
        "    individual_embeddings = []\n",
        "    for entry in message_time_output:\n",
        "      tokenized = tokenizer(entry['message'], padding=False, return_tensors=\"pt\")\n",
        "      with torch.no_grad():\n",
        "        embedding = model(input_ids=tokenized['input_ids'], attention_mask=tokenized['attention_mask']).last_hidden_state.mean(dim=1).cpu().numpy()[0]\n",
        "      time = entry['time']\n",
        "      composite_vector = np.concatenate((embedding, [float(time)], [last_message_time]))\n",
        "      individual_embeddings.append(composite_vector)\n",
        "    individual_embeddings = np.vstack((individual_embeddings))\n",
        "    padding_gen[:len(individual_embeddings),:] = individual_embeddings\n",
        "    mask_gen[:len(individual_embeddings)+1] = 1\n",
        "    dataset['Embeddings'].iloc[i] = padding_gen\n",
        "    dataset[f'Mask'].iloc[i] = mask_gen\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "oJq9mpc-YgJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variant of function 3 for scaling time"
      ],
      "metadata": {
        "id": "Gs02nH8np0Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINE FUNCTION THAT APPLIES THIS TO ALL RELEVENT STREAMS\n",
        "def embedding_generator(df,max_length,max_time):\n",
        "  dataset = df.copy()\n",
        "  dataset['Embeddings'] = \"\"\n",
        "  dataset['Mask'] = \"\"\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    padding_gen = np.zeros((max_length,768+1+1))\n",
        "    #The mask must also include the CLS token that will be added on later\n",
        "    mask_gen = np.zeros((max_length+1))\n",
        "    print(i)\n",
        "    last_message_time = dataset.iloc[i]['final_time']\n",
        "    message_time_output = json.loads(dataset.iloc[i]['messages_and_times'])\n",
        "    individual_embeddings = []\n",
        "    for entry in message_time_output:\n",
        "      tokenized = tokenizer(entry['message'], padding=False, return_tensors=\"pt\")\n",
        "      with torch.no_grad():\n",
        "        embedding = model(input_ids=tokenized['input_ids'], attention_mask=tokenized['attention_mask']).last_hidden_state.mean(dim=1).cpu().numpy()[0]\n",
        "      time = entry['time']\n",
        "      composite_vector = np.concatenate((embedding, [float(time)]/max_time, [last_message_time]/max_time))\n",
        "      individual_embeddings.append(composite_vector)\n",
        "    individual_embeddings = np.vstack((individual_embeddings))\n",
        "    padding_gen[:len(individual_embeddings),:] = individual_embeddings\n",
        "    mask_gen[:len(individual_embeddings)+1] = 1\n",
        "    dataset['Embeddings'].iloc[i] = padding_gen\n",
        "    dataset[f'Mask'].iloc[i] = mask_gen\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "lLrguKaMpzyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code prints out general summary statistics about the dataset we are analyzing. It also generates max length - the maximum number of messages that any person sent in a stream. This is used to set the number of tokens our transformer will use:"
      ],
      "metadata": {
        "id": "d5io-mFccJTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LOOK AT BOTH THE NUMBER OF SAMPLES AND THE MAX LENGTH FOR BOTH DONORS AND NON-DONORS\n",
        "\n",
        "paid_message_types = ['paid_message','paid_sticker','sponsorships_gift_purchase_announcement']\n",
        "placeholders = ', '.join('?' for _ in paid_message_types)\n",
        "\n",
        "#DONORS\n",
        "query = f\"\"\"\n",
        "SELECT ListofAuthors, COUNT(*)\n",
        "FROM DBdb\n",
        "WHERE URL = (?)\n",
        "  AND ListofMessages != ''\n",
        "  AND ListofAuthors IN (\n",
        "    SELECT DISTINCT ListofAuthors\n",
        "    FROM DBdb\n",
        "    WHERE URL = (?)\n",
        "      AND TypeofMessages IN ({placeholders})\n",
        "  )\n",
        "GROUP BY ListofAuthors\n",
        "\"\"\"\n",
        "num_donors = []\n",
        "max_donor_messages = []\n",
        "for stream in streams_of_interest:\n",
        "  df = pd.read_sql(query,con,params = [stream,stream] + paid_message_types)\n",
        "  max_donor_messages.append(df['COUNT(*)'].max())\n",
        "  num_donors.append(len(df))\n",
        "\n",
        "\n",
        "#NON DONORS\n",
        "query = f\"\"\"\n",
        "SELECT ListofAuthors, COUNT(*)\n",
        "FROM DBdb\n",
        "WHERE URL = (?)\n",
        "  AND ListofMessages != ''\n",
        "  AND ListofAuthors IN (\n",
        "    SELECT DISTINCT ListofAuthors\n",
        "    FROM DBdb\n",
        "    WHERE URL = (?)\n",
        "      AND TypeofMessages NOT IN ({placeholders})\n",
        "  )\n",
        "GROUP BY ListofAuthors\n",
        "\"\"\"\n",
        "num_nondonors = []\n",
        "max_nondonor_messages = []\n",
        "for stream in streams_of_interest:\n",
        "  df = pd.read_sql(query,con,params = [stream,stream] + paid_message_types)\n",
        "  max_nondonor_messages.append(df['COUNT(*)'].max())\n",
        "  num_nondonors.append(len(df))\n",
        "\n",
        "total_donor_samples = sum(num_donors)\n",
        "total_nondonor_samples = sum(num_nondonors)\n",
        "print('Total number of donor samples:{}'.format(total_donor_samples))\n",
        "print('Total number of nondonor samples:{}'.format(total_nondonor_samples))\n",
        "max_length = max(max_donor_messages + max_nondonor_messages)\n",
        "print('Max length:{}'.format(max_length))\n",
        "\n",
        "\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT MAX(ListofTimes)\n",
        "FROM DBdb\n",
        "WHERE URL = (?)\n",
        "\"\"\"\n",
        "\n",
        "last_time = []\n",
        "for stream in streams_of_interest:\n",
        "  df = pd.read_sql(query,con,params = [stream,stream] + paid_message_types)\n",
        "  last_time.append(df['COUNT(*)'].max())\n",
        "  num_nondonors.append(len(df))"
      ],
      "metadata": {
        "id": "I-MTzE2VcYNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT ListofTimes\n",
        "FROM DBdb\n",
        "WHERE URL = (?)\n",
        "\"\"\"\n",
        "\n",
        "last_time = []\n",
        "for stream in streams_of_interest:\n",
        "  df = pd.read_sql(query,con,params = [stream,])\n",
        "  if len(df['ListofTimes'].values)!= 0:\n",
        "      last_time.append(max(df['ListofTimes'].values.astype(float)))\n",
        "max_time = max(last_time)\n",
        "max_time"
      ],
      "metadata": {
        "id": "-L-JyLfHge4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the define functions to generate a test/training set of the desired size"
      ],
      "metadata": {
        "id": "dvLPsr8CdWjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GET ALL THE RELEVENT DATA FROM THE SQL DATABASES\n",
        "all_donors_df, all_non_donors_df, all_last_message_time = data_aggregator(streams_of_interest)\n",
        "\n",
        "#DEFINE THE DESIRED NUMBER OF SAMPLES\n",
        "num_samples = 2500\n",
        "num_train = 2000\n",
        "#SAMPLE AND SPLIT INTO TEST AND TRAINING DATASETS\n",
        "dataset_donor = all_donors_df.sample(n=num_samples)\n",
        "dataset_donor_train = dataset_donor.iloc[:num_train]\n",
        "dataset_donor_test = dataset_donor.iloc[num_train:]\n",
        "dataset_non_donor = all_non_donors_df.sample(n=num_samples)\n",
        "dataset_non_donor_train = dataset_non_donor.iloc[:num_train]\n",
        "dataset_non_donor_test = dataset_non_donor.iloc[num_train:]\n",
        "\n",
        "#GENERATE A DATAFRAME WITH THE TEST AND TRAINING DATA\n",
        "dataset_donor_test['label'] = 1\n",
        "dataset_donor_train['label'] = 1\n",
        "dataset_non_donor_test['label'] = 0\n",
        "dataset_non_donor_train['label'] = 0\n",
        "training_dataset = pd.concat([dataset_donor_train, dataset_non_donor_train], ignore_index=True)\n",
        "test_dataset = pd.concat([dataset_donor_test, dataset_non_donor_test], ignore_index=True)\n",
        "\n",
        "#GENERATE EMBEDDINGS FOR THE TRAINING AND TEST DATASETS\n",
        "embedded_training = embedding_generator(training_dataset,max_length,max_time)\n",
        "embedded_test = embedding_generator(test_dataset,max_length,max_time)"
      ],
      "metadata": {
        "id": "dfshHTsscX1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_test.to_pickle('/content/drive/MyDrive/SQLDATABASES/testing_embeddings_0612024_scaled_time.pkl')\n",
        "embedded_training.to_pickle('/content/drive/MyDrive/SQLDATABASES/training_embeddings_0612024_scaled_time.pkl')"
      ],
      "metadata": {
        "id": "FPv4_GWG6zyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_test.to_pickle('/content/drive/MyDrive/SQLDATABASES/testing_embeddings_0612024.pkl')"
      ],
      "metadata": {
        "id": "a5WV3zyySWno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_training.to_pickle('/content/drive/MyDrive/SQLDATABASES/training_embeddings_0612024.pkl')"
      ],
      "metadata": {
        "id": "IF5jFipSUl7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_test['Embeddings'].iloc[0]"
      ],
      "metadata": {
        "id": "tGcR-GfzefeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_training['Embeddings'].iloc[0]"
      ],
      "metadata": {
        "id": "VlgVooYbeHaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}