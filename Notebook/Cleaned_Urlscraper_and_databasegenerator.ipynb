{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNnTPlOgURecxxOEtg3P+BZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobisreallyme/TestRepo/blob/main/Cleaned_Urlscraper_and_databasegenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwGJvkMkbOP6"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver_manager\n",
        "!pip install odfpy\n",
        "!pip install google-colab-selenium\n",
        "!pip install chat_downloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
        "from selenium.webdriver.edge.service import Service\n",
        "import time\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import google_colab_selenium as gs\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uUEYe4qycDuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to provide the link to your ods file containing the urls/database names and locations you need to create, as well as a youtube api key so we can extract the dates and times."
      ],
      "metadata": {
        "id": "wA8zZ3SjdZbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ODS_FILE_PATH = 'ODS_File.ods'\n",
        "API_KEY_INIT = 'Your API Key'"
      ],
      "metadata": {
        "id": "8IzuFADPdYft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is for generating the list of urls of livestreams produced by a channel, along with a list of dates/times/titles"
      ],
      "metadata": {
        "id": "D4lLR6o7fn9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GET THE ODS FILE CONTAINING THE URLS AND THE LOCATIONS + NAMES OF THE ODS OF URLS YOU WANT TO CREATE\n",
        "data_frame_channels = pd.read_excel(ODS_FILE_PATH)\n",
        "channel_url_all = data_frame_channels.iloc[:,0].tolist()\n",
        "spreadsheet_name_all = data_frame_channels.iloc[:,1].tolist()\n",
        "\n",
        "#FUNCTION TO SCRAP THE CHANNEL URL\n",
        "def get_livestreamurls_fromchannel(channel_url):\n",
        "    #INITIALIZE WEBDRIVER\n",
        "    driver = gs.Chrome()\n",
        "    #OPEN CHANNEL URL\n",
        "    driver.get(channel_url)\n",
        "\n",
        "    #LOAD THE VIDEOS, BY SCROLLING TO THE BOTTOM OF THE PAGE\n",
        "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "        time.sleep(2)\n",
        "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "    #WAIT FOR LIVESTREAM URLS TO LOAD\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "    videos = wait.until(EC.presence_of_all_elements_located(\n",
        "        (By.CSS_SELECTOR, 'a.yt-simple-endpoint.style-scope.ytd-rich-grid-media')\n",
        "    ))\n",
        "\n",
        "    #EXTRACT AND PRINT URLS\n",
        "    urls = [video.get_attribute('href') for video in videos]\n",
        "\n",
        "    #APPEND THE URLS\n",
        "    urls_all = []\n",
        "    for url in urls:\n",
        "        if not not url:\n",
        "            urls_all.append(url)\n",
        "            print(url)\n",
        "\n",
        "    #CLOSE BROWSER\n",
        "    driver.quit()\n",
        "    return urls_all\n",
        "\n",
        "\n",
        "#USE THE GOOGLE API TO GET DATES/TIMES/TITLES OFF YOUTUBE\n",
        "def API_get_title_date(url):\n",
        "    API_KEY = API_KEY_INIT\n",
        "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "    temp_ids = url.split('https://www.youtube.com/watch?v=')[1]\n",
        "    temp_ids = temp_ids.split('&t=')[0]\n",
        "    video_id = temp_ids\n",
        "    video_response = youtube.videos().list(\n",
        "        part='snippet',\n",
        "        id=video_id\n",
        "    ).execute()\n",
        "\n",
        "    upload_date_str = video_response['items'][0]['snippet']['publishedAt']\n",
        "    upload_date = datetime.strptime(upload_date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "    video_name = video_response['items'][0]['snippet']['title']\n",
        "    video_status = video_response['items'][0]['snippet']['liveBroadcastContent']\n",
        "\n",
        "    return video_name,upload_date,video_status\n",
        "\n",
        "\n",
        "#LOOP TO EXTRACT THE DATE GIVEN THE ABOVE FUNCTIONS AND WRITE THEM INTO AN ODS FILE. IF ANY CANNOT BE FETCHED, NOTE THAT IT HAS BEEN EXCLUDED\n",
        "for index_temp in range(0,len(channel_url_all)):\n",
        "    channel_url = channel_url_all[index_temp]\n",
        "    channel_url\n",
        "    spreadsheet_name = spreadsheet_name_all[index_temp]\n",
        "    spreadsheet_name\n",
        "\n",
        "    urls_all = get_livestreamurls_fromchannel(channel_url)\n",
        "\n",
        "\n",
        "    video_name_all = []\n",
        "    upload_date_all = []\n",
        "    video_url_all = []\n",
        "\n",
        "\n",
        "    for urls_temp in urls_all:\n",
        "        video_name,upload_date,video_status = API_get_title_date(urls_temp)\n",
        "        if not video_status == 'upcoming':\n",
        "            video_name_all.append(video_name)\n",
        "            upload_date_all.append(upload_date)\n",
        "            video_url_all.append(urls_temp)\n",
        "        else:\n",
        "            print('excluded')\n",
        "            print(urls_temp)\n",
        "\n",
        "\n",
        "    temp_sort_order = np.argsort(upload_date_all)\n",
        "\n",
        "\n",
        "    sorted_dates_all = [upload_date_all[index].strftime(\"%Y-%m-%dT%H:%M:%SZ\") for index in temp_sort_order]\n",
        "    sorted_titles_all = [video_name_all[index] for index in temp_sort_order]\n",
        "    sorted_urls_all = [video_url_all[index] for index in temp_sort_order]\n",
        "\n",
        "\n",
        "    aggregated_streams = {}\n",
        "    aggregated_streams['titles'] = sorted_titles_all\n",
        "    aggregated_streams['urls'] = sorted_urls_all\n",
        "    aggregated_streams['dates'] = sorted_dates_all\n",
        "\n",
        "\n",
        "    dataframe_aggregated_streams = pd.DataFrame(aggregated_streams)\n",
        "\n",
        "\n",
        "    with pd.ExcelWriter(spreadsheet_name,engine=\"odf\") as writer:\n",
        "        dataframe_aggregated_streams.to_excel(writer)"
      ],
      "metadata": {
        "id": "LGpyvLUobZnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the generated urls, extract the livestream chat data and store as an SQLite3 database of the form:\n",
        "\n",
        "id/URL/TypeofMessages/ListofTimes/ListofBadges/ListofAuthors/ListofMessages/DonationAmount/DonationCurrency\n",
        "\n",
        "Extraction is done using the chat_downloader library developed by xenova (Joshua Lochner)"
      ],
      "metadata": {
        "id": "NaOIjZ5lf1kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from chat_downloader import ChatDownloader"
      ],
      "metadata": {
        "id": "ZdjwABvogilh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHANGE THESE ACCORDING TO HOW YOU WANT TO NAME THE DATABASE\n",
        "spreadsheet_name = r\"url_list.ods\"\n",
        "db_document = r'Db_file.dbs'\n",
        "\n",
        "Alldataframe = pd.read_excel(spreadsheet_name)\n",
        "\n",
        "ListofStreamUrls = Alldataframe.iloc[:,2].tolist()"
      ],
      "metadata": {
        "id": "w1ClHOUNhbfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Allchat = []\n",
        "\n",
        "def GetChatfromstream(urlinput):\n",
        "    try:\n",
        "        return ChatDownloader().get_chat(urlinput, message_groups=['messages', 'superchat'])\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "import sqlite3\n",
        "conn = sqlite3.connect(db_document)\n",
        "batchsize = 100\n",
        "c = conn.cursor()\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS DBdb (\n",
        "                        id INTEGER PRIMARY KEY,\n",
        "                        URL TEXT,\n",
        "                        TypeofMessages TEXT,\n",
        "                        ListofTimes TEXT,\n",
        "                        ListofBadges TEXT,\n",
        "                        ListofAuthors TEXT,\n",
        "                        ListofMessages TEXT,\n",
        "                        DonationAmount TEXT,\n",
        "                        DonationCurrency TEXT\n",
        "                        )''')\n",
        "\n",
        "\n",
        "i = 1\n",
        "j = 1\n",
        "for URL in ListofStreamUrls:\n",
        "\n",
        "    chat = GetChatfromstream(URL)\n",
        "\n",
        "    batch = []\n",
        "    print(j)\n",
        "    if chat!=[]:\n",
        "        for message in chat:\n",
        "            try:\n",
        "                test = message['time_in_seconds']\n",
        "                if message['message_type'] == 'paid_message':\n",
        "                    ListofMessages = message['message']\n",
        "                    DonationAmount = message['money']['amount']\n",
        "                    DonationCurrency = message['money']['currency']\n",
        "                elif message['message_type'] == 'paid_sticker':\n",
        "                    ListofMessages = 'NULL'\n",
        "                    DonationAmount = message['money']['amount']\n",
        "                    DonationCurrency = message['money']['currency']\n",
        "                else:\n",
        "                    ListofMessages = message['message']\n",
        "                    DonationAmount = 'NULL'\n",
        "                    DonationCurrency = 'NULL'\n",
        "\n",
        "                ListofAuthors = message['author']['name']\n",
        "\n",
        "                if 'badges' not in message['author']:\n",
        "                    ListofBadges = 'NULL'\n",
        "                else:\n",
        "                    ListofBadges = message['author']['badges'][0]['title']\n",
        "\n",
        "                ListofTimes = message['time_in_seconds']\n",
        "                TypeofMessages = message['message_type']\n",
        "\n",
        "            except KeyError:\n",
        "\n",
        "                print('KeyError Encountered')\n",
        "                break\n",
        "\n",
        "            batch.append((i,URL,TypeofMessages,ListofTimes,ListofBadges,ListofAuthors,ListofMessages,DonationAmount,DonationCurrency))\n",
        "            i=i+1\n",
        "\n",
        "            if len(batch) >= batchsize:\n",
        "                c.executemany('''INSERT INTO DBdb\n",
        "                            (id, URL, TypeofMessages, ListofTimes, ListofBadges, ListofAuthors, ListofMessages, DonationAmount, DonationCurrency)\n",
        "                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''', batch)\n",
        "                batch = []\n",
        "\n",
        "        if batch:\n",
        "            c.executemany('''INSERT INTO DBdb\n",
        "                            (id, URL, TypeofMessages, ListofTimes, ListofBadges, ListofAuthors, ListofMessages, DonationAmount, DonationCurrency)\n",
        "                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''', batch)\n",
        "        j = j + 1\n",
        "        conn.commit()\n",
        "\n",
        "    print('save completed')"
      ],
      "metadata": {
        "id": "CfpmdZLehSSx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
