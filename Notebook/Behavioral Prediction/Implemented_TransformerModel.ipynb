{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDLOLxX2yS8zeGWmGKRgJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobisreallyme/TestRepo/blob/main/Implemented_TransformerModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "mvbqYH7-Y6P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can remove this if you are not running in colab"
      ],
      "metadata": {
        "id": "3ywZEmR7xn3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V62X6bDkY75D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Data (change to your directory)"
      ],
      "metadata": {
        "id": "FJrQskcrZXFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuuBq26eYyJf"
      },
      "outputs": [],
      "source": [
        "testing_embeddings = pd.read_pickle('/content/drive/MyDrive/SQLDATABASES/testing_embeddings_0612024_scaled_time.pkl')\n",
        "training_embeddings = pd.read_pickle('/content/drive/MyDrive/SQLDATABASES/training_embeddings_0612024_scaled_time.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset Object"
      ],
      "metadata": {
        "id": "NORBS3ome2e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE DATASET AND LOAD INTO BATCHES\n",
        "class Embedded(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.labels = df['label'].values\n",
        "        self.embeddings = np.stack(df['Embeddings'].values)\n",
        "        self.mask = df['Mask'].values\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.embeddings[index], dtype=torch.float32), torch.tensor(self.labels[index], dtype=torch.long), torch.tensor(self.mask[index]==0,dtype=torch.bool)\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "dataset_test = Embedded(testing_embeddings)\n",
        "dataset_training = Embedded(training_embeddings)\n"
      ],
      "metadata": {
        "id": "M1LxRWDPZt86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Model. You can change the number of attention\n"
      ],
      "metadata": {
        "id": "zYpOLP0je99V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#DEFINE MODEL\n",
        "#\n",
        "class AddCLS(nn.Module):\n",
        "    def __init__(self, dim_data):\n",
        "        super(AddCLS, self).__init__()\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim_data))\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        return x\n",
        "\n",
        "class DimensionalityReduction(nn.Module):\n",
        "    def __init__(self, dim_data, dim_latent):\n",
        "        super(DimensionalityReduction, self).__init__()\n",
        "        self.dim_red = nn.Linear(dim_data, dim_latent)\n",
        "    def forward(self, x):\n",
        "        return self.dim_red(x)\n",
        "\n",
        "class MultiheadAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim_latent, dim_latent_2, n_heads):\n",
        "        super(MultiheadAttentionBlock, self).__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(dim_latent, n_heads, batch_first=True)\n",
        "        self.layer_norm1 = nn.LayerNorm(dim_latent)\n",
        "        self.layer_norm2 = nn.LayerNorm(dim_latent)\n",
        "        self.linear_1 = nn.Linear(dim_latent, dim_latent_2)\n",
        "        self.linear_2 = nn.Linear(dim_latent_2, dim_latent)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.multihead_attn(x, x, x, key_padding_mask = mask)\n",
        "        x = x + attn_output\n",
        "        x = self.layer_norm1(x)\n",
        "        #x = self.dropout(x)\n",
        "\n",
        "        ff_output = self.relu(self.linear_1(x))\n",
        "        ff_output = self.linear_2(ff_output)\n",
        "        x = x + ff_output\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Classification(nn.Module):\n",
        "    def __init__(self, dim_latent):\n",
        "        super(Classification, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.classifier = nn.Linear(dim_latent ,2)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(x[:, 0, :])\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, dim_data, dim_latent, dim_latent_2, n_heads):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.add_cls = AddCLS(dim_data)\n",
        "        self.dim_red = DimensionalityReduction(dim_data, dim_latent)\n",
        "        self.attn_block = MultiheadAttentionBlock(dim_latent, dim_latent_2, n_heads)\n",
        "        self.classifier = Classification(dim_latent)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.add_cls(x)\n",
        "        x = self.dim_red(x)\n",
        "        x = self.attn_block(x, mask)\n",
        "        x = self.attn_block(x, mask)\n",
        "        x = self.attn_block(x, mask)\n",
        "        x = self.attn_block(x, mask)\n",
        "        x = self.attn_block(x, mask)\n",
        "        x = self.attn_block(x, mask)\n",
        "        x = self.attn_block(x, mask)\n",
        "        x = self.attn_block(x, mask)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XQ-gHk1pbL6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DONT RUN THIS! You can change out the final classifier above from a linear to non linear layer, depending on desired training speed/interpretibility"
      ],
      "metadata": {
        "id": "S1j1VlkxfBj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOU CAN ADD NON-LINEARITY TO THE FINAL CLASSIFCATION LAYER, IT ACCELERATES TRAINING SLIGHTLY\n",
        "#Non-linear classifier\n",
        "class Classification(nn.Module):\n",
        "    def __init__(self, dim_latent):\n",
        "        super(Classification, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.classifier = nn.Linear(dim_latent ,2)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(x[:, 0, :])\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "#Linear classifier\n",
        "class Classification(nn.Module):\n",
        "    def __init__(self, dim_latent):\n",
        "        super(Classification, self).__init__()\n",
        "        self.classifier = nn.Linear(dim_latent ,2)\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x[:, 0, :])"
      ],
      "metadata": {
        "id": "fYdPmWYILATF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to save model with top accuracy. CHANGE file_path ACCORDING TO YOUR NEEDS!"
      ],
      "metadata": {
        "id": "pWFGNDqNq9gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, val_loss, accuracy, file_path='/content/drive/MyDrive/ModelTrained/ModelClassifyDonors/Modelmaxvalaccuracy.pth'):\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': val_loss,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "    torch.save(state, file_path)\n",
        "    print(f'Model saved at epoch {epoch} with accuracy {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "1xFTrS1Vqjm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell to train model and save best model along with all relevant data"
      ],
      "metadata": {
        "id": "YmPHfOoWrJnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SETUP MODEL\n",
        "dim_data = 770\n",
        "dim_latent = 256\n",
        "dim_latent_2 = 512\n",
        "n_heads = 1\n",
        "model = TransformerModel(dim_data, dim_latent, dim_latent_2, n_heads)\n",
        "\n",
        "# LOAD DATA\n",
        "train_loader = DataLoader(dataset_training, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=10, shuffle=False)\n",
        "\n",
        "# SET UP TRAINING LOOP FOR MODEL\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
        "n_epochs = 200\n",
        "all_loss = []\n",
        "all_acc = []\n",
        "all_val_loss = []\n",
        "all_val_acc = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    corrects = (preds == targets).sum().item()\n",
        "    accuracy = corrects / targets.size(0)\n",
        "    return accuracy\n",
        "\n",
        "#RUN\n",
        "#ADDITIONAL VALIDATION ACCURACY STEP AS A CHECK\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "val_acc = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs, targets, mask = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "        outputs = model(inputs,mask)\n",
        "        loss_value = loss_fn(outputs, targets)\n",
        "        val_loss += loss_value.item()\n",
        "        val_acc += calculate_accuracy(outputs, targets)\n",
        "val_loss /= len(test_loader)\n",
        "val_acc /= len(test_loader)\n",
        "all_val_loss.append(val_loss)\n",
        "all_val_acc.append(val_acc)\n",
        "max_val_acc = val_acc\n",
        "print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "#TRAINING LOOP\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for batch in train_loader:\n",
        "        inputs, targets, mask = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs,mask)\n",
        "        loss_value = loss_fn(outputs, targets)\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss_value.item()\n",
        "        epoch_acc += calculate_accuracy(outputs, targets)\n",
        "    epoch_loss /= len(train_loader)\n",
        "    epoch_acc /= len(train_loader)\n",
        "    all_loss.append(epoch_loss)\n",
        "    all_acc.append(epoch_acc)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets, mask = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            outputs = model(inputs,mask)\n",
        "            loss_value = loss_fn(outputs, targets)\n",
        "            val_loss += loss_value.item()\n",
        "            val_acc += calculate_accuracy(outputs, targets)\n",
        "    val_loss /= len(test_loader)\n",
        "    val_acc /= len(test_loader)\n",
        "    all_val_loss.append(val_loss)\n",
        "    all_val_acc.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > max_val_acc:\n",
        "        max_val_acc = val_acc\n",
        "        save_checkpoint(model, optimizer, epoch, val_loss, val_acc)\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "id": "M8l_pK1-bh64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Loss and accuracy"
      ],
      "metadata": {
        "id": "jlp0B3VlxwpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "ax1.plot(all_val_loss[:100], 'r')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Validation Loss (cross entropy)', color='r', fontsize=15)\n",
        "ax1.tick_params(axis='y', labelcolor='r')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(all_val_acc[:100], 'b')\n",
        "ax2.set_ylabel('Validation Accuracy', color='b', fontsize=15)\n",
        "ax2.tick_params(axis='y', labelcolor='b')\n"
      ],
      "metadata": {
        "id": "AOX0zk7xjT_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Plot just loss"
      ],
      "metadata": {
        "id": "dY101ehOx1mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.dpi'] = 1000\n",
        "plt.plot(all_val_loss[:100])\n",
        "plt.xlabel('Epoch', fontsize=15)\n",
        "plt.ylabel('Validation Loss (cross entropy)', fontsize=15)\n"
      ],
      "metadata": {
        "id": "rC-Fq-fRj12b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
