{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOU5u7zf5m92KXSeTGxWE3B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobisreallyme/TestRepo/blob/main/Cleaned_PretrainingDistilbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeCP1yl6bM4X"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from datasets import concatenate_datasets\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import math"
      ],
      "metadata": {
        "id": "PoVf3L9ZbP1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streams_of_interest_file = r'YourStreamsOfInterest.ods'\n",
        "database_filename = r'YourDatabase.db'"
      ],
      "metadata": {
        "id": "cBublCn_bRQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINE STREAMS OF INTEREST\n",
        "Alldataframe = pd.read_excel(streams_of_interest_file)\n",
        "\n",
        "ListofStreamUrls = Alldataframe.iloc[:,2].tolist()\n",
        "ListofStreamNames = Alldataframe.iloc[:,1].tolist()\n",
        "\n",
        "streams_of_interest = []\n",
        "for i in range(93,300+93,2):\n",
        "    streams_of_interest.append(ListofStreamUrls[i])\n",
        "\n",
        "\n",
        "#CONNECT TO DATABASE OF INTEREST\n",
        "all_messages = []\n",
        "all_times = []\n",
        "all_names = []\n",
        "donor_names = []\n",
        "ChatfileCM = sqlite3.connect(database_filename)\n",
        "CursorCM = ChatfileCM.cursor()\n",
        "\n",
        "#GET DATA OFF DATABASE. REMOVE ALL STREAMS WHERE THERE IS NO DATA\n",
        "for index,url_temp in enumerate(streams_of_interest):\n",
        "\n",
        "    all_messages_temp = []\n",
        "    all_times_temp = []\n",
        "    all_names_temp = []\n",
        "\n",
        "    Messagetype = 'text_message'\n",
        "    CursorCM.execute(\"SELECT ListofMessages FROM DBdb WHERE URL = ? ORDER BY id\", (url_temp,))\n",
        "    StreamoneText0 = CursorCM.fetchall()\n",
        "    Testmessages = []\n",
        "    for i in range(0,len(StreamoneText0)):\n",
        "        Testmessages.append(StreamoneText0[i][0])\n",
        "    all_messages_temp.append(Testmessages)\n",
        "\n",
        "\n",
        "    CursorCM.execute(\"SELECT ListofTimes FROM DBdb WHERE URL = ? ORDER BY id\", (url_temp,))\n",
        "    StreamoneText0 = CursorCM.fetchall()\n",
        "    Testtimes = []\n",
        "    for i in range(0,len(StreamoneText0)):\n",
        "        Testtimes.append(StreamoneText0[i][0])\n",
        "    all_times_temp.append(Testtimes)\n",
        "\n",
        "\n",
        "    CursorCM.execute(\"SELECT ListofAuthors FROM DBdb WHERE URL = ? ORDER BY id\", (url_temp,))\n",
        "    StreamoneText0 = CursorCM.fetchall()\n",
        "    Testnames = []\n",
        "    for i in range(0,len(StreamoneText0)):\n",
        "        Testnames.append(StreamoneText0[i][0])\n",
        "    all_names_temp.append(Testnames)\n",
        "\n",
        "    if len(all_messages_temp[0]) > 0:\n",
        "        all_messages.append(all_messages_temp[0])\n",
        "        all_times.append(all_times_temp[0])\n",
        "        all_names.append(all_names_temp[0])\n",
        "    else:\n",
        "        print(index)\n",
        "\n",
        "\n",
        "#FUNCTION TO SORT MESSAGES AND SUCH BY TIME\n",
        "def message_sorter(times,messages,names):\n",
        "    zipped = zip(np.array(times).astype(float),messages,names)\n",
        "    sorted_zipped = sorted(zipped, key=lambda x: x[0])\n",
        "    sorted_times, sorted_messages,sorted_names = zip(*sorted_zipped)\n",
        "    return sorted_times,sorted_messages,sorted_names\n",
        "\n",
        "#SORT DATA\n",
        "sorted_times = []\n",
        "sorted_messages = []\n",
        "sorted_names = []\n",
        "\n",
        "for index,temp_mess in enumerate(all_messages):\n",
        "    temp_sorted_times,temp_sorted_messages,temp_sorted_names = message_sorter(all_times[index],all_messages[index],all_names[index])\n",
        "    sorted_times.append(list(temp_sorted_times))\n",
        "    sorted_messages.append(list(temp_sorted_messages))\n",
        "    sorted_names.append(list(temp_sorted_names))\n",
        "\n",
        "\n",
        "#REMOVE A RANDOM SET OF DATA FOR PERPLEXITY CALCS LATER\n",
        "#TOKENIZE FUNCTION\n",
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"messages\"])\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "    return result\n",
        "\n",
        "#LOAD MODEL FROM HUGGINGFACE\n",
        "model_checkpoint = \"distilroberta-base\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "lm_datasets_aggregate = []\n",
        "test_dataset_aggregate = []\n",
        "chunk_size = 128\n",
        "\n",
        "for index,temp_sorted_mess in enumerate(sorted_messages):\n",
        "\n",
        "    #REMOVE ALL NONES\n",
        "    sorted_messages_processed = []\n",
        "    for index,message in enumerate(temp_sorted_mess):\n",
        "        if message!=None:\n",
        "            sorted_messages_processed.append(message)\n",
        "\n",
        "\n",
        "    #SPLIT THE DATASET INTO A SECTION FOR PERPLEXITY ON ORGINAL DATASET FORMAT AND THE MAIN DATASET (THIS IS A SECOND TEST DATASET TO MAKE SURE THAT CONCATENATION DOESN'T RESULT IN LOSS OF PREDICTION ABILITY ON ORIGINAL DATASET)\n",
        "    length_removed = int(len(sorted_messages_processed)*0.0005)\n",
        "    indices_to_remove = np.random.choice(len(sorted_messages_processed), length_removed, replace=False)\n",
        "    sorted_message_main = [message_temp for index,message_temp in enumerate(sorted_messages_processed) if index not in indices_to_remove]\n",
        "    test_dataset = [message_temp for index,message_temp in enumerate(sorted_messages_processed) if index in indices_to_remove]\n",
        "\n",
        "    test_dataset_aggregate = test_dataset_aggregate + test_dataset\n",
        "\n",
        "    temp_dict = {\"messages\":sorted_message_main}\n",
        "    temp_dataset = Dataset.from_dict(temp_dict)\n",
        "\n",
        "    #BATCH TOKENIZE\n",
        "    tokenized_datasets = temp_dataset.map(\n",
        "        tokenize_function, batched=True, remove_columns=[\"messages\"]\n",
        "    )\n",
        "\n",
        "    def group_texts(examples):\n",
        "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "        #DROP LAST CHUNK IF IT'S SMALLER THAN CHUNK SIZE\n",
        "        total_length = (total_length // chunk_size) * chunk_size\n",
        "        result = {\n",
        "            k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "        return result\n",
        "\n",
        "    lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
        "    lm_datasets_aggregate.append(lm_datasets)\n",
        "\n",
        "\n",
        "#PULL ALL THE DATASETS TOGETHER INTO ONE DATASET OBJECT\n",
        "lm_datasets_concatenated = lm_datasets_aggregate[0]\n",
        "for index in range(1,len(lm_datasets_aggregate)):\n",
        "    lm_datasets_concatenated = concatenate_datasets([lm_datasets_concatenated,lm_datasets_aggregate[index]])\n",
        "\n",
        "#MASK THE DATA FOR TRAINING AND TESTING\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
        "def insert_random_mask(batch):\n",
        "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
        "    masked_inputs = data_collator(features)\n",
        "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
        "\n",
        "#SPLIT INTO TRAIN AND TEST DATASETS\n",
        "train_size = int(total_number_samples*0.9)\n",
        "test_size = int(total_number_samples*0.1)\n",
        "\n",
        "downsampled_dataset = lm_datasets_concatenated.train_test_split(\n",
        "    train_size=train_size, test_size=test_size, seed=42\n",
        ")\n",
        "downsampled_dataset\n",
        "\n",
        "\n",
        "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
        "eval_dataset = downsampled_dataset[\"test\"].map(\n",
        "    insert_random_mask,\n",
        "    batched=True,\n",
        "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
        ")\n",
        "\n",
        "\n",
        "#PROCESS THE ORIGINAL LENGTH DATASET\n",
        "def tokenize_function_2(examples):\n",
        "    return tokenizer(examples[\"messages\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "test_dict = {\"messages\":test_dataset_aggregate}\n",
        "test_dataset_dict = Dataset.from_dict(test_dict)\n",
        "\n",
        "tokenized_test_dataset = test_dataset_dict.map(\n",
        "    tokenize_function_2, batched=True, remove_columns=[\"messages\"]\n",
        ")\n",
        "\n",
        "\n",
        "eval_dataset_2 = tokenized_test_dataset.map(\n",
        "    insert_random_mask,\n",
        "    batched=True,\n",
        "    remove_columns=['input_ids', 'attention_mask'],\n",
        ")\n",
        "\n",
        "\n",
        "eval_dataset_2 = eval_dataset_2.rename_columns(\n",
        "    {\n",
        "        \"masked_input_ids\": \"input_ids\",\n",
        "        \"masked_attention_mask\": \"attention_mask\",\n",
        "        \"masked_labels\": \"labels\",\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "eval_dataset = eval_dataset.rename_columns(\n",
        "    {\n",
        "        \"masked_input_ids\": \"input_ids\",\n",
        "        \"masked_attention_mask\": \"attention_mask\",\n",
        "        \"masked_labels\": \"labels\",\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(\n",
        "    downsampled_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
        ")\n",
        "\n",
        "\n",
        "eval_dataloader_2 = DataLoader(\n",
        "    eval_dataset_2, batch_size=batch_size, collate_fn=default_data_collator\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "#SEND MODEL TO THE GPU\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "#SET UP SAVEPATH\n",
        "default_path_tosave = r\"C:\\Users\\BobsBrain\\Contrast_Learning\\Model_from_concatentation_4_26_2024_\"\n",
        "\n",
        "\n",
        "#INITIALIZE TRAINING PARAMETERS\n",
        "from torch.optim import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")\n",
        "\n",
        "from transformers import get_scheduler\n",
        "num_train_epochs = 10\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "#TRAIN AND EVALUATE MODEL\n",
        "\n",
        "model.eval()\n",
        "#CALCULATE THE INITIAL MODEL PERPLEXITY ON CONCATENATED DATA\n",
        "losses = []\n",
        "for step, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    loss = outputs.loss\n",
        "    losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
        "\n",
        "losses = torch.cat(losses)\n",
        "losses = losses[: len(eval_dataset)]\n",
        "try:\n",
        "    perplexity = math.exp(torch.mean(losses))\n",
        "except OverflowError:\n",
        "    perplexity = float(\"inf\")\n",
        "\n",
        "print(f\"Perplexity on Concat: {perplexity}\")\n",
        "\n",
        "#CALCULATE THE INITIAL MODEL PERPLEXITY ON NON CONCATENATED DATA\n",
        "losses = []\n",
        "for step, batch in enumerate(eval_dataloader_2):\n",
        "    with torch.no_grad():\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    loss = outputs.loss\n",
        "    losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
        "\n",
        "losses = torch.cat(losses)\n",
        "losses = losses[: len(eval_dataset_2)]\n",
        "try:\n",
        "    perplexity = math.exp(torch.mean(losses))\n",
        "except OverflowError:\n",
        "    perplexity = float(\"inf\")\n",
        "\n",
        "print(f\"Perplexity: {perplexity}\")\n",
        "\n",
        "#SAVE INITIAL MODEL\n",
        "\n",
        "\n",
        "#MODEL TRAINING LOOP\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "\n",
        "    #SAVE THE MODEL AFTER TRAINING\n",
        "    model.save_pretrained(default_path_tosave + str(epoch+11))\n",
        "\n",
        "    #EVALUATION ON CONCAT DATASET\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
        "\n",
        "    losses = torch.cat(losses)\n",
        "    losses = losses[: len(eval_dataset)]\n",
        "    try:\n",
        "        perplexity = math.exp(torch.mean(losses))\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "\n",
        "    print(f\">>> Epoch {epoch}: Perplexity on Concat: {perplexity}\")\n",
        "\n",
        "\n",
        "\n",
        "    #EVALUATION ON NON CONCAT DATASET\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader_2):\n",
        "        with torch.no_grad():\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
        "\n",
        "    losses = torch.cat(losses)\n",
        "    losses = losses[: len(eval_dataset_2)]\n",
        "    try:\n",
        "        perplexity = math.exp(torch.mean(losses))\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "\n",
        "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NWqf6jbxbS39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
