{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOY2LNJDPDoWo2AiCkvQHHM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobisreallyme/TestRepo/blob/main/Cleaned_DB_updater.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chat_downloader"
      ],
      "metadata": {
        "id": "o4HUYrUKj9TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ODS_file_toupdate = 'Your_DB_file_toupdate.ods'"
      ],
      "metadata": {
        "id": "ekhqn_SBknJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRNwOuMLj2gw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sqlite3\n",
        "from googleapiclient.discovery import build\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from chat_downloader import ChatDownloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXTRACT THE URLS FROM THE DOCUMENT\n",
        "def get_urls_database(db_document):\n",
        "    Chatfile = sqlite3.connect(db_document)\n",
        "    Cursorfile = Chatfile.cursor()\n",
        "    Cursorfile.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
        "    table_name = Cursorfile.fetchall()\n",
        "    table_name = table_name[0][0]\n",
        "    query = \"SELECT URL FROM \" + table_name\n",
        "    all_urls = Cursorfile.execute(query)\n",
        "\n",
        "    url_list = []\n",
        "    for temp_urls in all_urls:\n",
        "        temp_urls = temp_urls[0]\n",
        "        url_list.append(temp_urls.split('&t=')[0])\n",
        "    unique_urls = list(set(url_list))\n",
        "\n",
        "    Cursorfile.close()\n",
        "\n",
        "    return unique_urls,table_name,url_list\n",
        "\n",
        "#GET CHAT FROM STREAM\n",
        "def GetChatfromstream(urlinput):\n",
        "    try:\n",
        "        return ChatDownloader().get_chat(urlinput, message_groups=['messages', 'superchat'])\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "#CODE TO UPDATE DATABASE\n",
        "def update_database(new_streams,db_document,table_name):\n",
        "    Allchat = []\n",
        "\n",
        "    import sqlite3\n",
        "    conn = sqlite3.connect(db_document)\n",
        "    batchsize = 100\n",
        "    c = conn.cursor()\n",
        "\n",
        "    id_values = []\n",
        "    query_1 = \"SELECT id FROM \" + table_name\n",
        "    c.execute(query_1)\n",
        "    id_values_temp = c.fetchall()\n",
        "\n",
        "    for i in range(0,len(id_values_temp)):\n",
        "        id_values.append(id_values_temp[i][0])\n",
        "\n",
        "    istart = max(id_values)+1\n",
        "\n",
        "    i = istart\n",
        "    j = 1\n",
        "\n",
        "\n",
        "    query_2 = '''INSERT INTO {}\n",
        "    (id, URL, TypeofMessages, ListofTimes, ListofBadges, ListofAuthors, ListofMessages, DonationAmount, DonationCurrency)\n",
        "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)'''.format(table_name)\n",
        "\n",
        "\n",
        "    for URL in new_streams:\n",
        "\n",
        "        chat = GetChatfromstream(URL)\n",
        "\n",
        "        batch = []\n",
        "        print(j)\n",
        "        if chat!=[]:\n",
        "            for message in chat:\n",
        "                try:\n",
        "                    test = message['time_in_seconds']\n",
        "                    if message['message_type'] == 'paid_message':\n",
        "                        ListofMessages = message['message']\n",
        "                        DonationAmount = message['money']['amount']\n",
        "                        DonationCurrency = message['money']['currency']\n",
        "                    elif message['message_type'] == 'paid_sticker':\n",
        "                        ListofMessages = 'NULL'\n",
        "                        DonationAmount = message['money']['amount']\n",
        "                        DonationCurrency = message['money']['currency']\n",
        "                    else:\n",
        "                        ListofMessages = message['message']\n",
        "                        DonationAmount = 'NULL'\n",
        "                        DonationCurrency = 'NULL'\n",
        "\n",
        "                    ListofAuthors = message['author']['name']\n",
        "\n",
        "                    if 'badges' not in message['author']:\n",
        "                        ListofBadges = 'NULL'\n",
        "                    else:\n",
        "                        ListofBadges = message['author']['badges'][0]['title']\n",
        "\n",
        "                    ListofTimes = message['time_in_seconds']\n",
        "                    TypeofMessages = message['message_type']\n",
        "\n",
        "                except KeyError:\n",
        "\n",
        "                    print('KeyError Encountered')\n",
        "                    break\n",
        "\n",
        "                batch.append((i,URL,TypeofMessages,ListofTimes,ListofBadges,ListofAuthors,ListofMessages,DonationAmount,DonationCurrency))\n",
        "                i=i+1\n",
        "\n",
        "                if len(batch) >= batchsize:\n",
        "                    c.executemany(query_2, batch)\n",
        "                    batch = []\n",
        "\n",
        "            if batch:\n",
        "                c.executemany(query_2, batch)\n",
        "            j = j + 1\n",
        "            conn.commit()\n",
        "\n",
        "#GET THE FILES TO UPDATE\n",
        "inputs_to_check = pd.read_excel(ODS_file_toupdate,header=None)\n",
        "all_excel_file = inputs_to_check.iloc[:,1].tolist()\n",
        "all_db_document = inputs_to_check.iloc[:,2].tolist()\n",
        "\n",
        "\n",
        "#RUN UPDATING ON EACH FILE\n",
        "for temp_index in range(len(all_excel_file)):\n",
        "    db_document = all_db_document[temp_index]\n",
        "    excel_file = all_excel_file[temp_index]\n",
        "    Alldataframe = pd.read_excel(excel_file)\n",
        "    ListofStreamUrls = Alldataframe.iloc[:,2].tolist()\n",
        "\n",
        "    unique_urls,table_name,url_list = get_urls_database(db_document)\n",
        "\n",
        "    last_database_index = [index for index,url_temp in enumerate(ListofStreamUrls) if url_temp in unique_urls]\n",
        "\n",
        "    future_streams  = ListofStreamUrls[max(last_database_index)+1:len(ListofStreamUrls)]\n",
        "    print(future_streams)\n",
        "\n",
        "    new_streams = [url_temp for index_temp, url_temp in enumerate(future_streams) if url_temp not in unique_urls]\n",
        "    print(new_streams)\n",
        "\n",
        "    update_database(new_streams,db_document,table_name)"
      ],
      "metadata": {
        "id": "tikYUJlJkMOh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
